{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be9f7653",
      "metadata": {
        "id": "be9f7653"
      },
      "source": [
        "**Heidelberg University**\n",
        "\n",
        "**Data Science  Group**\n",
        "    \n",
        "Prof. Dr. Michael Gertz  \n",
        "\n",
        "Ashish Chouhan, Satya Almasian, John Ziegler, Jayson Salazar, Nicolas Reuter\n",
        "    \n",
        "January 16, 2024\n",
        "    \n",
        "Natural Language Processing with Transformers\n",
        "\n",
        "Winter Semster 2023/2024     \n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258e9648",
      "metadata": {
        "id": "258e9648"
      },
      "source": [
        "# **Assignment 4: Question Answering**\n",
        "Group 16:   \n",
        "\n",
        "         \n",
        "        Xiaoqing Cai\n",
        "\n",
        "        Qiaowen Hu  \n",
        "\n",
        "        Binwu Wang  \n",
        "\n",
        "        Wenzhuo Chen\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc27ad9e",
      "metadata": {
        "id": "fc27ad9e"
      },
      "source": [
        "### **Submission Guidelines**\n",
        "\n",
        "- Solutions need to be uploaded as a **single** Jupyter notebook. You will find several pre-filled code segments in the notebook, your task is to fill in the missing cells.\n",
        "- For the written solution, use LaTeX in markdown inside the same notebook. Do **not** hand in a separate file for it.\n",
        "- Download the .zip file containing the dataset but do **not** upload it with your solution.\n",
        "- It is sufficient if one person per group uploads the solution to Moodle, but make sure that the full names of all team members are given in the notebook.\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HETm7VsBkmLq",
      "metadata": {
        "id": "HETm7VsBkmLq"
      },
      "source": [
        "## **Task 1: Retrieval Augmented Generation (RAG)** ( 4.5 + 3 + 4 + 3 + 1.5 = 16 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ODkKBIRkrfe",
      "metadata": {
        "id": "0ODkKBIRkrfe"
      },
      "source": [
        "In this task, we look at using the open source `Llama-13b-chat` model for creating a RAG system. You must first apply for access to Llama 2 models via [this](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) form (access is typically granted within a few hours). etrieval augmented generation you also need to request to use the model on Hugging Face by going to the [model](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) card. ***Note that the emails you provide for your Hugging Face account must match the email you used to request Llama 2.***\n",
        "\n",
        "The final piece that you need is a Hugging Face authentication token. You can find such a token by going to the `setting` in your Hugging Face profile, under the `Access Token` menu you can generate a new token.\n",
        "\n",
        "To store the document you will need a free Pinecone [API key](https://app.pinecone.io/).\n",
        "Make sure you have these pieces ready before starting to work on this task.\n",
        "\n",
        "----\n",
        "When ready, let's start by downloading the necessary packages.\n",
        "\n",
        "It is advised to proceed with this notebook with a GPU (if you are on Colab make sure that a GPU environment is activated.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yO1OwuHTMo75",
      "metadata": {
        "id": "yO1OwuHTMo75"
      },
      "source": [
        "Place all the access tokens in the `.env` file and upload it to the working directory (if you are running this notebook locally, you can change the path to fit your working directory). Please use the following format:\n",
        "\n",
        "\n",
        "```\n",
        "HF_AUTH= \"Hugging Face Authentication Key\"\n",
        "PINECONE_API_KEY=\"Pincone API Key\"\n",
        "PINECONE_ENVIRONMENT=\"Pinecone Environment\"\n",
        "```\n",
        "\n",
        "Run the cell below to load the access tokens into the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Dxt-e1bg73b",
      "metadata": {
        "id": "1Dxt-e1bg73b"
      },
      "outputs": [],
      "source": [
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EqmnxgpALbVV",
      "metadata": {
        "id": "EqmnxgpALbVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# load environment variables from .env file\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yE3EOlhTrKo6",
      "metadata": {
        "id": "yE3EOlhTrKo6"
      },
      "outputs": [],
      "source": [
        "%pip install litellm\n",
        "%pip install -qU trulens_eval pydantic fastapi kaleido python-multipart uvicorn cohere openai tiktoken \"llama-index\"\n",
        "%pip install transformers\n",
        "%pip install sentence-transformers\n",
        "%pip install pinecone-client\n",
        "%pip install datasets\n",
        "%pip install accelerate\n",
        "%pip install einops\n",
        "%pip install langchain\n",
        "%pip install xformers\n",
        "%pip install bitsandbytes\n",
        "%pip install matplotlib seaborn tqdm\n",
        "%pip install chromadb\n",
        "%pip install evaluate\n",
        "%pip install rouge_score\n",
        "%pip install bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dLDAPMVST2Ds",
      "metadata": {
        "id": "dLDAPMVST2Ds"
      },
      "source": [
        "\n",
        "\n",
        "## Subtask 1.1: Data Preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b9odZyFLDin",
      "metadata": {
        "id": "9b9odZyFLDin"
      },
      "source": [
        "We need a collection of documents to perform our retrieval on. To make it closer to your final project, you will be downloading and using a subset of the LangChain documentation. We get some of the `.html` files located on the site. The code below will download all HTML files from the links on the webpage into a `docs` directory. `-l1` limits the download to only the first level of depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JXe86ZKVFfrm",
      "metadata": {
        "id": "JXe86ZKVFfrm"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qiodFLkaLUsF",
      "metadata": {
        "id": "qiodFLkaLUsF"
      },
      "outputs": [],
      "source": [
        "!wget -r -l1 -A.html -P docs https://api.python.langchain.com/en/stable/langchain_api_reference.html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5DWa9M6JLb8O",
      "metadata": {
        "id": "5DWa9M6JLb8O"
      },
      "source": [
        " The docs are going to be used as input text for answering questions that a normal language model might not be aware of (LangChain docs is not necessarily part of its training data of Llama2). We can use LangChain itself to process these docs. Use the [ReadTheDocsLoader](https://python.langchain.com/docs/integrations/document_loaders/readthedocs_documentation) to load the docs from the `docs` folder.\n",
        "\n",
        " At the time of creating this notebook, there  `423` documents were downloaded. However, since the documentation is being updated regularly this number might be different for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nd8ufORKLVy0",
      "metadata": {
        "id": "Nd8ufORKLVy0"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "#### your code ####\n",
        "docs =\n",
        "#### your code ####\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yKs0_OrBMQ7M",
      "metadata": {
        "id": "yKs0_OrBMQ7M"
      },
      "source": [
        "Let's take a look at one of the documents. You see that LangChain has created a `Document` object. Look at the example below and fill in the cells to print out the text content and URL of the page (the URL of the page should starts with `https://`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vcpuudNRLV7k",
      "metadata": {
        "id": "vcpuudNRLV7k"
      },
      "outputs": [],
      "source": [
        "docs[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Q24oWkGM3IP",
      "metadata": {
        "id": "5Q24oWkGM3IP"
      },
      "outputs": [],
      "source": [
        "#### your code ####\n",
        "page_content=\n",
        "page_url=\n",
        "#### your code ####\n",
        "print(page_content)\n",
        "print(page_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OfZKdZJsMeyj",
      "metadata": {
        "id": "OfZKdZJsMeyj"
      },
      "source": [
        "As you can imagine the documents can be long and if multiple of them are required as context to answer questions, we need to take the document lengths into account.\n",
        "This is due to the fact that language models do not have unlimited context span. In our case, we plan to use Llama2 for this project, where the maximum token limit is 4096. This limit is not only the input but also takes the generated output into account, moreover, you need to leave room for the query and instructions as well. Therefore, it is important to chunk the longer documents into smaller-sized fragments.\n",
        "\n",
        "Based on your use case and how many contexts you plan to feed into the model the length of these fragments will differ.\n",
        "In this case, we choose to assign 2000 tokens to context and choose to generate the answer from 5 context fragments, which leaves us with 400 tokens per context fragment as the maximum chunk size.\n",
        "\n",
        "To count the number of tokens in a chunk, we need to load the correct tokenizer for Llama2. Fill the code cell below to load the correct tokenizer and use it to complete the function that counts the number of tokens per given chunk.\n",
        "\n",
        "**Hint:** you need to use your Hugging Face authentication token to load the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WjOHBqXLLWDA",
      "metadata": {
        "id": "WjOHBqXLLWDA"
      },
      "outputs": [],
      "source": [
        "#If you get an error here during the first import from the `transformers` package, restart the kernel and try again.\n",
        "#### your code ####\n",
        "\n",
        "tokenizer =\n",
        "#### your code ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G5GutpFdLWFu",
      "metadata": {
        "id": "G5GutpFdLWFu"
      },
      "outputs": [],
      "source": [
        "def token_len(text):\n",
        "  #### your code ####\n",
        "\n",
        "    #### your code ####"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2pdG2JeGPfcS",
      "metadata": {
        "id": "2pdG2JeGPfcS"
      },
      "source": [
        "Count the number of tokens for all documents and use it to compute minimum, maximum, and average token count statistics across all documents. Depending on how the documentation is updated by the time you run the cell below the numbers might slightly differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HdAdFPYyLWIM",
      "metadata": {
        "id": "HdAdFPYyLWIM"
      },
      "outputs": [],
      "source": [
        "#### your code ####\n",
        "token_counts =\n",
        "min_tokens=\n",
        "avg_tokens=\n",
        "max_tokens=\n",
        "#### your code ####\n",
        "print(f\"\"\"Min: {min_tokens}\n",
        "Avg: {avg_tokens}\n",
        "Max: {max_tokens}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iz9rYYRtMo2N",
      "metadata": {
        "id": "iz9rYYRtMo2N"
      },
      "source": [
        "Now we will use LangChain's built-in chunking functionality to split the text into smaller chunks. LangChain offers a variety of text splitters that you can check out [here](https://api.python.langchain.com/en/latest/langchain_api_reference.html#module-langchain.text_splitter).\n",
        "Use the general-purpose splitter that splits text by recursively looking at characters. Use this class to split the text into 400 token-sized chunks, where the length of each chunk is computed based on the `token_len` function. The length is not the only criterion for splitting, if any of these separators `'\\n\\n', '\\n', ' ', ''` is encountered, we will have a new chunk.\n",
        "Since splitting only based on maximum length might result in incoherent chunks for every consecutive chunk, let the chunk overlap by 50 tokens. This way,  we preserve some of the previous context while chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_RaaGMPPUd9",
      "metadata": {
        "id": "0_RaaGMPPUd9"
      },
      "outputs": [],
      "source": [
        "#### your code ####\n",
        "\n",
        "text_splitter =\n",
        "#### your code ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kS6uNIzpOD-I",
      "metadata": {
        "id": "kS6uNIzpOD-I"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(docs[100].page_content)\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-r29RUJ7PUg6",
      "metadata": {
        "id": "-r29RUJ7PUg6"
      },
      "outputs": [],
      "source": [
        "token_len(chunks[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SGaeBXo8ONLj",
      "metadata": {
        "id": "SGaeBXo8ONLj"
      },
      "source": [
        "The next step is to apply the splitting function to all the documents in our corpus and to save our chunks in a logical way. We also want to assign a unique ID to each chunk so we know which part of the documentation they come from. In the end, the corpus should be transformed into a list of dictionaries of the following format:\n",
        "\n",
        "\n",
        "```\n",
        "[\n",
        "    {\n",
        "        \"id\": \"glossary-0\",\n",
        "        \"text\": \"first chunk of the document glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"glossary-1\",\n",
        "        \"text\": \"second chunk of glossary\",\n",
        "        \"source\": \"https://langchain.readthedocs.io/en/latest/glossary.html\"\n",
        "    }\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "Construct the IDs by taking the name of the page before the suffix `.html` and appending a chronological number indicating which chunk it is.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBsNJBUNPUjC",
      "metadata": {
        "id": "pBsNJBUNPUjC"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in tqdm(docs):\n",
        "  #### your code ####\n",
        "    url =\n",
        "    uid =\n",
        "    chunks =\n",
        "\n",
        "  #### your code ####\n",
        "len(documents) # once again this value might differ based on how the LangChain documentation is updated"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hnx_s1bBQFQM",
      "metadata": {
        "id": "Hnx_s1bBQFQM"
      },
      "source": [
        "For the next steps, we require a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V64EmH9bLWNO",
      "metadata": {
        "id": "V64EmH9bLWNO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(documents)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CGbju_bkOQoL",
      "metadata": {
        "id": "CGbju_bkOQoL"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VTrg2hMrO0vF",
      "metadata": {
        "id": "VTrg2hMrO0vF"
      },
      "source": [
        "## Subtask 1.2: Document Embedding Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q_DSEsWIT9eI",
      "metadata": {
        "id": "q_DSEsWIT9eI"
      },
      "source": [
        "In this task, we initialize the embedding pipeline to transform the chunks into vector embeddings using Hugging Face and LangChain. These embeddings are used for similarity search between the query and the chunks to retrieve the most relevant chunks.\n",
        "  We will use the `sentence-transformers/all-MiniLM-L6-v2` model for embedding, which is a rather small model that you can easily run on Colab. Initialize the model using `HuggingFaceEmbeddings` to use Hugging Face via Langchain. The encoding batch size should be 32, and make sure that the model is placed on the correct device, otherwise, this can take a long time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DYpEw-IKUTqK",
      "metadata": {
        "id": "DYpEw-IKUTqK"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import os\n",
        "import pinecone\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HI3CJ5bbUTsT",
      "metadata": {
        "id": "HI3CJ5bbUTsT"
      },
      "outputs": [],
      "source": [
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "device = 'cuda:0' # make sure you are on gpu\n",
        "docs = [\n",
        "    \"An example document\",\n",
        "    \"A second document as an example\"\n",
        "]\n",
        "### your code ###\n",
        "embed_model =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_U73Hj93V0_-",
      "metadata": {
        "id": "_U73Hj93V0_-"
      },
      "source": [
        "Embed the example documents using the model you created and check the output.\n",
        "The output should be a list of lists, containing the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Qt3SDKoUvKx",
      "metadata": {
        "id": "9Qt3SDKoUvKx"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "embeddings =\n",
        "### your code ###\n",
        "print(\"number of docs:\",len(embeddings))\n",
        "print(\"dimension of docs:\",len(embeddings[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2D8sr-g9W8cw",
      "metadata": {
        "id": "2D8sr-g9W8cw"
      },
      "source": [
        "Now we use the embedding pipeline created above to store the embeddings in a Pinecone vector index. First, lets setup the Pinecone environment, collect your API key and environment name from the environment variables, and initiate Pinecone with them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tycv_q6RW8tO",
      "metadata": {
        "id": "tycv_q6RW8tO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_9tvVVQ7X2pV",
      "metadata": {
        "id": "_9tvVVQ7X2pV"
      },
      "source": [
        "Initialize the index `rag-assignment` inside Pinecone. Use the cosine similarity as similarity metric. Keep in mind that if you run this multiple times on a free tier, where only one index is allowed, you need to remove the index created to make room for a new one (Pinecone index gets archived automatically after 14 days of inactivity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_ZA_2J5XyTD",
      "metadata": {
        "id": "g_ZA_2J5XyTD"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-assignment'\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rxfs9gshYfKx",
      "metadata": {
        "id": "rxfs9gshYfKx"
      },
      "source": [
        "Lets take a look at the index you created. As of now the index should be empty but have the correct embedding dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gNnN9mEyYWHB",
      "metadata": {
        "id": "gNnN9mEyYWHB"
      },
      "outputs": [],
      "source": [
        "index_name = 'rag-assignment'\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "woQKBXfobZMg",
      "metadata": {
        "id": "woQKBXfobZMg"
      },
      "source": [
        "Process the dataset in batches of `32` and push the vectors to the Pinecone index. Your index should include the IDs and embeddings for each chunk. As metadata, pass the original text as `text` and the URL as `source` (no need to add the `https`). We use this metadata later to retrieve the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K8vjCSVKa8Y6",
      "metadata": {
        "id": "K8vjCSVKa8Y6"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "  ### your code ###\n",
        "\n",
        "    ids =\n",
        "    texts =\n",
        "    embeds =\n",
        "\n",
        "    ### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kz2UZVj8ff_L",
      "metadata": {
        "id": "kz2UZVj8ff_L"
      },
      "source": [
        "Now if we look at the index statistics we should have vectors of dimension `384`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cH9Zq6azfgJm",
      "metadata": {
        "id": "cH9Zq6azfgJm"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NK-2Scv8qqYV",
      "metadata": {
        "id": "NK-2Scv8qqYV"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bVrpkiJhHAF",
      "metadata": {
        "id": "5bVrpkiJhHAF"
      },
      "source": [
        "## Subtask 1.3: Text Generation Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPykfVaHkIkO",
      "metadata": {
        "id": "ZPykfVaHkIkO"
      },
      "source": [
        "So far we have our index ready and a way to find the most similar chunks to our query. Now, we need a way to generate the answer from the retrieved chunks. For this purpose, we use the `text-generation` pipeline from Hugging Face (refer to the Hugging Face [tutorial](https://moodle.uni-heidelberg.de/pluginfile.php/1286642/mod_resource/content/1/HuggingFace.ipynb)) and load it into LangChain using a wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CL_PhWM3lEnj",
      "metadata": {
        "id": "CL_PhWM3lEnj"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import os\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-13b-chat-hf'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A3l34T5T3eEN",
      "metadata": {
        "id": "A3l34T5T3eEN"
      },
      "source": [
        "Quantization techniques reduce memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn’t be able to fit into memory, and thus speeds up inference.\n",
        "To make the process of model quantization more accessible, Hugging Face has seamlessly integrated with the [Bitsandbytes](https://huggingface.co/docs/accelerate/usage_guides/quantization) library.\n",
        "\n",
        "Define a config from `Bitsandbytes` that enables 4-bit quantization and set the nested quantization to `true`. This changes the datatype from float 32 (default) to normalized float 4 datatype to contain 4 bits of information.\n",
        "Additionally, add a compute type to store weights in 4-bits, but the computation to happen in 16-bit (bfloat16).\n",
        "Moreover, set the `bnb_4bit_use_double_quant` to true, which uses a second quantization after the first one to save an additional 0.4 bits per parameter.\n",
        "Refer to [here](https://huggingface.co/docs/transformers/main_classes/quantization) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mXNTODoOlFjX",
      "metadata": {
        "id": "mXNTODoOlFjX"
      },
      "outputs": [],
      "source": [
        "  ### your code ###\n",
        "bitsAndBites_config =\n",
        "  ### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wea-kVMF4Kvf",
      "metadata": {
        "id": "Wea-kVMF4Kvf"
      },
      "source": [
        "Use your Hugging Face token to load the correct model configuration using the `transformers` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7U47CyIk4EUz",
      "metadata": {
        "id": "7U47CyIk4EUz"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "model_config =\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V8CNl7G1SsUC",
      "metadata": {
        "id": "V8CNl7G1SsUC"
      },
      "source": [
        "Load the model for text generation (pay attention to the model type) using the configuration file you have defined, with the specified quantization, and set the `trust_remote_code` flag to `true`. Another flag that is useful for large mode is  `device_map=\"auto\"`. By setting this flag, Accelerate will determine where to put each layer to maximize the use of GPUs and offload the rest on the CPU, or even the hard drive if you don’t have enough GPU RAM (or CPU RAM).\n",
        "\n",
        "It will take a while for the model to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TUzuGsv61cBA",
      "metadata": {
        "id": "TUzuGsv61cBA"
      },
      "outputs": [],
      "source": [
        "#Loading the model will take some time, (roughly 5 min)\n",
        "### your code ###\n",
        "model =\n",
        "### your code ###\n",
        "model.eval()# we only use the model for inference\n",
        "print(f\"Model loaded \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-brElhysTZVZ",
      "metadata": {
        "id": "-brElhysTZVZ"
      },
      "source": [
        "You can even check the memory footprint of your model using the `get_memory_footprint` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zWDTgyKhlLLQ",
      "metadata": {
        "id": "zWDTgyKhlLLQ"
      },
      "outputs": [],
      "source": [
        "model.get_memory_footprint()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ViFoo0vlSFMp",
      "metadata": {
        "id": "ViFoo0vlSFMp"
      },
      "source": [
        "The next thing we need to do is initialize a `text-generation` pipeline with Hugging Face that uses the Llama2 model to generate some text, given some input. We will then use this pipeline inside LangChain to build our question-answering system.\n",
        "`text-generation` pipeline generates text from a language model conditioned on a given input. The pipeline is similar to other Hugging Face pipelines and requires two things that we must initialize:\n",
        "\n",
        "1.   A language model, in this case, it will be `meta-llama/Llama-2-13b-chat-hf`.\n",
        "2.   A tokenizer for the language model.\n",
        "\n",
        "LangChain expects the full-text outputs, therefore set the `return_full_text` to true. You can also pass additional generation parameters to the model.\n",
        "Since we want the questions to be answered mainly based on the retrieved chunks, let's set the model temperature to a low value of 0.01 to reduce randomness. Additionally, add a repetition penalty of 1.1 to stop the model from repeating itself and the maximum number of generation tokens to 512."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aFv-9-lPCJO",
      "metadata": {
        "id": "1aFv-9-lPCJO"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "generate_text =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZvdSEw9PZ-n9",
      "metadata": {
        "id": "ZvdSEw9PZ-n9"
      },
      "source": [
        "We provide the language model a general question to make sure our pipeline is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FpuCkc77RF53",
      "metadata": {
        "id": "FpuCkc77RF53"
      },
      "outputs": [],
      "source": [
        "sample_input=\"Explain to me the difference between alligator and crocodile.\"\n",
        "### your code ###\n",
        "\n",
        "generated_text=\n",
        "### your code ###\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2QtViqTBSsPS",
      "metadata": {
        "id": "2QtViqTBSsPS"
      },
      "source": [
        "Use the LangChain Hugging Face wrapper, as subset of [LLM chain](https://python.langchain.com/docs/modules/chains/foundational/llm_chain) to create an interface for the text generation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6-spabNmRMM2",
      "metadata": {
        "id": "6-spabNmRMM2"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "llm =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9EFZBGNgTM20",
      "metadata": {
        "id": "9EFZBGNgTM20"
      },
      "source": [
        "To confirm that it works the same way, use the sample input to generate text using the llm chain. The input should be passed as the `prompt` to the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rDToa1YOTFEX",
      "metadata": {
        "id": "rDToa1YOTFEX"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mn7-zQxeVhWF",
      "metadata": {
        "id": "mn7-zQxeVhWF"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.3}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VRwTtnQEWq5h",
      "metadata": {
        "id": "VRwTtnQEWq5h"
      },
      "source": [
        "## Subtask 1.4: Question Answering Chain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wr9QzQvYXDBS",
      "metadata": {
        "id": "wr9QzQvYXDBS"
      },
      "source": [
        "For Retrieval Augmented Generation (RAG) in LangChain, we need to initialize either a `RetrievalQA` or `RetrievalQAWithSourcesChain` object.\n",
        "\n",
        "`RetrievalQA` is a method for question-answering tasks, utilizing an index to retrieve relevant documents or text chunks, it is suitable for straightforward Q&A applications.\n",
        "\n",
        "`RetrievalQAWithSourcesChain` is an extension of RetrievalQA that chains together multiple sources of information, providing context and the source for answers.\n",
        "\n",
        " For both of these, we need an LLM and a Pinecone index. For LangChain to be able to use the Pinecone index, we need to initialize it through the LangChain vector store.\n",
        "\n",
        " **Hint**: You need to explicitly tell the vector storage where to find the original text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RQIR_Gb6Wrfs",
      "metadata": {
        "id": "RQIR_Gb6Wrfs"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "### your code ###\n",
        "vectorstore =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mS9d51dyptJd",
      "metadata": {
        "id": "mS9d51dyptJd"
      },
      "source": [
        "Let's try a query that is specific to the LangChain documentation and see which chunks are relevant. Use the vector storage defined above to find the top-3 chunks related to the given query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sQeWNPDqXdJe",
      "metadata": {
        "id": "sQeWNPDqXdJe"
      },
      "outputs": [],
      "source": [
        "query = 'what is a LangChain Agent?'\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gEMooqALXqOt",
      "metadata": {
        "id": "gEMooqALXqOt"
      },
      "source": [
        "Now use the `vectorstore` and `llm` to initialize the `RetrievalQA` object, which showcases question answering over an index. `RetrievalQA` is a document chain, these are useful for summarizing documents, answering questions about documents, extracting information from documents, and more. All such chains operate with 4 different chain types:\n",
        "\n",
        "\n",
        "1.   `stuff`: it takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM.\n",
        "2.   `refine`: it constructs a response by looping over the input documents and iteratively updating its answer. It is well-suited for tasks that require analyzing more documents than can fit in the model’s context.\n",
        "3. `map_reduce`:  it first applies an LLM chain to each document individually (the Map step), treating the chain output as a new document. It then passes all the new documents to a separate combined documents chain to get a single output (the Reduce step).\n",
        "4. `map_re_rank`: it runs an initial prompt on each document that not only tries to complete a task but also gives a score for how certain it is in its answer. The highest-scoring response is returned.\n",
        "\n",
        "For this assignment, we focus only on the first type. Make sure to set the `verbose` to `true`, so we can see the different stages of processing that happens while answering a question (you might need to set this parameter more than once). As mentioned before, we want our retrieve to input top-5 most similiar chunks to the query to generate an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lYnUrFnaXqU0",
      "metadata": {
        "id": "lYnUrFnaXqU0"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "\n",
        "rag_pipeline =\n",
        "\n",
        "### your code ###\n",
        "query='what is a LangChain Agent?'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O4Mo0JNMqSC7",
      "metadata": {
        "id": "O4Mo0JNMqSC7"
      },
      "source": [
        "First, we try to answer the question only using Llama2. As you see the answer is not convincing as it does not have access to the LangChain documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jGk2ZsUVYiJ0",
      "metadata": {
        "id": "jGk2ZsUVYiJ0"
      },
      "outputs": [],
      "source": [
        "llm(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W8U5Vce0qdch",
      "metadata": {
        "id": "W8U5Vce0qdch"
      },
      "source": [
        "Now use the Pipeline from above and see how the answer changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1KaQyXKQYn57",
      "metadata": {
        "id": "1KaQyXKQYn57"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pJ2JxOL4aqM2",
      "metadata": {
        "id": "pJ2JxOL4aqM2"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.4}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inIFkOziTce3",
      "metadata": {
        "id": "inIFkOziTce3"
      },
      "source": [
        "## Subtask 1.5: Conversational Retrieval Chain\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "btxcJlEITiDt",
      "metadata": {
        "id": "btxcJlEITiDt"
      },
      "source": [
        "We can also extend our retrieval chain to be able to remember the previous questions and answer the current question by looking at the previous context.\n",
        "The important part of a conversational model is conversation memory, which transforms the stateless language model to be able to remember previous interactions, e.g., similiar to ChatGPT. In this subtask, we will use LangChain to create a conversational memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DYHPVo-9TiGU",
      "metadata": {
        "id": "DYHPVo-9TiGU"
      },
      "source": [
        "To implement the memory we use `ConversationalRetrievalChain`.\n",
        "This chain takes in chat history (a list of messages) and new questions and then returns an answer to that question. The algorithm for this chain consists of three parts:\n",
        "\n",
        "1. Use the chat history and the new question to create a new question that contains the information from the previous context.\n",
        "\n",
        "2. This new question is passed to the retriever and relevant documents are returned.\n",
        "\n",
        "3. The retrieved documents are passed to an LLM to generate a final response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GLv_d-RYyNYL",
      "metadata": {
        "id": "GLv_d-RYyNYL"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "chat_history = []\n",
        "\n",
        "### your code ###\n",
        "qa_conversation =\n",
        "result =\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6HxivGwyv2x",
      "metadata": {
        "id": "S6HxivGwyv2x"
      },
      "outputs": [],
      "source": [
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yd6tqKhg24dL",
      "metadata": {
        "id": "yd6tqKhg24dL"
      },
      "source": [
        "Change the chat history to contain the previous question and answer pair and ask a follow-up question.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36vj0R9Fyyd1",
      "metadata": {
        "id": "36vj0R9Fyyd1"
      },
      "outputs": [],
      "source": [
        "follow_up=\"What are tools and toolkits?\"\n",
        "\n",
        "### your code ###\n",
        "chat_history =\n",
        "result =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EvlY2MtH0xo3",
      "metadata": {
        "id": "EvlY2MtH0xo3"
      },
      "source": [
        "This is the previous context that was fed in alongside the new question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09zxWnojzKEu",
      "metadata": {
        "id": "09zxWnojzKEu"
      },
      "outputs": [],
      "source": [
        "chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DSlQYeWG04Gj",
      "metadata": {
        "id": "DSlQYeWG04Gj"
      },
      "source": [
        "The current question is answered by knowing that the tools and toolkits are referring to a LangChain Agent, which was part of the previous question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5FP4_WAbzMSQ",
      "metadata": {
        "id": "5FP4_WAbzMSQ"
      },
      "outputs": [],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LQUW_iFs3q3m",
      "metadata": {
        "id": "LQUW_iFs3q3m"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 1.5}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0KDwOSuNPBe-",
      "metadata": {
        "id": "0KDwOSuNPBe-"
      },
      "source": [
        "## **Task 2: Advanced RAG Techniques and Evaluation (4 + 5 = 9 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cN6pjlW7PBhQ",
      "metadata": {
        "id": "cN6pjlW7PBhQ"
      },
      "source": [
        "Now that you have successfully implemented your first RAG system, we dive into more advanced techniques and learn how to evaluate your methods using metrics you learned during the lecture. We focus on evaluation with an already annotated dataset. To this end, we load a small subset of [NarrativeQA](https://huggingface.co/datasets/narrativeqa), which is an English-language dataset of stories and corresponding questions designed to test reading comprehension, especially on long documents. We only load 30 samples from the data, as you will see in the upcoming sections, answer generation takes quite some time. In actual setting, it is advised to use a much larger set to obtain statistically significant results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wM2dodC2PAlr",
      "metadata": {
        "id": "wM2dodC2PAlr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"satyaalmasian/narrativeqa_subset\",split=\"train[:30]\")\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N0HDF2M1fz4D",
      "metadata": {
        "id": "N0HDF2M1fz4D"
      },
      "source": [
        "Since we already used our free index in Pinecone for the previous task, we use Chroma, an open-source vector database, instead. As opposed to Pinecone, Chroma creates a collection on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hZ8kqj95hqvo",
      "metadata": {
        "id": "hZ8kqj95hqvo"
      },
      "outputs": [],
      "source": [
        "from langchain.docstore.document import Document\n",
        "documents=[ doc[\"text\"] for doc in dataset[\"document\"]]\n",
        "questions=[quest for quest in dataset[\"question\"]]\n",
        "answers=[ans for ans in dataset[\"answers\"]]\n",
        "documents=list(set(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fj0X3PqHIFpQ",
      "metadata": {
        "id": "fj0X3PqHIFpQ"
      },
      "outputs": [],
      "source": [
        "docs= [Document(page_content=doc, metadata={\"source\": \"local\"}) for doc in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3BDIuYQkJB0U",
      "metadata": {
        "id": "3BDIuYQkJB0U"
      },
      "source": [
        "The number of documents is smaller  than the number of questions and answers and each document is used as a reference for multiple questions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FbWqRg12JLJn",
      "metadata": {
        "id": "FbWqRg12JLJn"
      },
      "outputs": [],
      "source": [
        "print(len(docs))\n",
        "print(len(questions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cD69GYOjjJuk",
      "metadata": {
        "id": "cD69GYOjjJuk"
      },
      "source": [
        "##Subtask 2.1: Build Contextual Compression in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D-OyjUQGhGPq",
      "metadata": {
        "id": "D-OyjUQGhGPq"
      },
      "source": [
        "Let's split our documents using the TextSplitter from Task 1 and embed them inside the Chroma database with the embedding model of the previous task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7NGs-RzhQa7",
      "metadata": {
        "id": "M7NGs-RzhQa7"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "all_splits =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hHknAFVBf5ia",
      "metadata": {
        "id": "hHknAFVBf5ia"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "### your code ###\n",
        "vectordb =\n",
        "retriever =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RbSBLyg364Ie",
      "metadata": {
        "id": "RbSBLyg364Ie"
      },
      "outputs": [],
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "r_docs = retriever.get_relevant_documents(questions[2]['text'])\n",
        "r_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5YIitJ47Km7p",
      "metadata": {
        "id": "5YIitJ47Km7p"
      },
      "source": [
        "First, make a simple RAG pipeline that works on top of the Chroma retriever. This retriever should be similar to the previous task. However, since we want to use it for a large number of questions, remove the `verbose` parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CTp5BOapKami",
      "metadata": {
        "id": "CTp5BOapKami"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "### your code ###\n",
        "rag_simple=\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I8EzQFiNRja9",
      "metadata": {
        "id": "I8EzQFiNRja9"
      },
      "source": [
        "We look at an example question and compare the answer by RAG to the gold answer from the dataset. Note that the answers can contain multiple lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afGjILYMKfCK",
      "metadata": {
        "id": "afGjILYMKfCK"
      },
      "outputs": [],
      "source": [
        "rag_simple(questions[2]['text']) #ignore the warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jvjAzg42R2WE",
      "metadata": {
        "id": "jvjAzg42R2WE"
      },
      "outputs": [],
      "source": [
        "answers[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mNXD6PaUSlLE",
      "metadata": {
        "id": "mNXD6PaUSlLE"
      },
      "source": [
        "Apply the `rag_simple` pipeline to all the question in your corpus and accumulate the answers. **It should take around 10 minutes on a T4 GPU on Colab**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uyTM0QD2KzwW",
      "metadata": {
        "id": "uyTM0QD2KzwW"
      },
      "outputs": [],
      "source": [
        "simple_answers=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0q3EbAw_hV5h",
      "metadata": {
        "id": "0q3EbAw_hV5h"
      },
      "source": [
        "Libraries such as LangChain and [Llamaindex](https://www.llamaindex.ai/) provide a variety of retrieval strategies for building a RAG system. In this subtask, you will use one of these variations called **contextual compression**. This method aims to extract only the relevant information from documents, reducing the need for expensive language model calls and improving response quality. Contextual compression consists of two parts:\n",
        "\n",
        "\n",
        "1.  **Base retriever:** retrieves the initial set of documents based on the query. This is similar to the retriever from the previous task.\n",
        "2.   **Document compressor:** processes these documents to extract the relevant content. We use `LLMChainExtractor`, which will iterate over the initially returned documents and extract from each only the content that is relevant to the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RJ0UzPIVPAoc",
      "metadata": {
        "id": "RJ0UzPIVPAoc"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor,LLMChainFilter\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "### your code ###\n",
        "\n",
        "compression_retriever =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qMPaRwoeuvDX",
      "metadata": {
        "id": "qMPaRwoeuvDX"
      },
      "source": [
        "Let's take a look at an example of compression retriever works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AK9xhW8JPAqo",
      "metadata": {
        "id": "AK9xhW8JPAqo"
      },
      "outputs": [],
      "source": [
        "print(\"Fist question in the set:\",questions[2]['text'])\n",
        "compressed_docs = compression_retriever.get_relevant_documents(questions[2]['text'])\n",
        "compressed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eWAPRPqCS3WP",
      "metadata": {
        "id": "eWAPRPqCS3WP"
      },
      "source": [
        "Look at the output and try out several different questions by yourself. Does the compressed output make sense?\n",
        "\n",
        "Compare this to the previous **simple** approach. Which one, in your opinion, is better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1GVulJfSu3lt",
      "metadata": {
        "id": "1GVulJfSu3lt"
      },
      "source": [
        "Finally, we use the new retriever with the Llama2 model from the previous task to create the context compressor RAG pipeline. The code below should be similiar to what you did in the previous task. Once again, make sure to turn off the `verbose` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKu2Dhr1PAtM",
      "metadata": {
        "id": "PKu2Dhr1PAtM"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_compressor =\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DDtrO4z2pAD9",
      "metadata": {
        "id": "DDtrO4z2pAD9"
      },
      "outputs": [],
      "source": [
        "rag_compressor(questions[2]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8PfY6ya8z-1T",
      "metadata": {
        "id": "8PfY6ya8z-1T"
      },
      "source": [
        "Now we can use the pipeline to generate answers for all the questions in our dataset. **It should take around 20 minutes on a T4 GPU on Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eofiL90APAvl",
      "metadata": {
        "id": "eofiL90APAvl"
      },
      "outputs": [],
      "source": [
        "compressor_answers=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ck8Ktt6Zns5",
      "metadata": {
        "id": "6ck8Ktt6Zns5"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 2.1}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RlA3IDsn0Oxa",
      "metadata": {
        "id": "RlA3IDsn0Oxa"
      },
      "source": [
        "##Subtask 2.2. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E274c-Yc0gAr",
      "metadata": {
        "id": "E274c-Yc0gAr"
      },
      "source": [
        "Since we have access to ground truth answers, we can use various evaluation metrics from the literature. In this task, we explore three metrics:\n",
        "\n",
        "\n",
        "1.   **BLEU:** BLEU score stands for Bilingual Evaluation Understudy and is a precision-based metric developed\n",
        "for evaluating machine translation. BLEU scores a candidate by computing the\n",
        "number of n-grams in the candidate that also appear\n",
        "in a reference. The n can vary, in this task we compute for n=4.\n",
        "2.   **ROUGE:** ROUGE score stands for Recall-Oriented Understudy for Gisting Evaluation and is an F-measure metric designed for\n",
        "evaluating translation and summarization. There are a number of variants of ROUGE.\n",
        "3. **BERTScore:** BERTScore first obtains BERT representation of each word in the candidate and reference by feeding the candidate\n",
        "and reference through a BERT model separately.\n",
        "An alignment is then computed between candidate\n",
        "and reference words by computing pairwise cosine\n",
        "similarity. This alignment is then aggregated in to\n",
        "precision and recall scores before being aggregated\n",
        "into a (modified) F1 score that is weighted using\n",
        "inverse-document-frequency values.\n",
        "\n",
        "Luckily, Hugging Face has an implementation for all these metrics. Use the `evaluate` library to load the metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VT57xstfUtAL",
      "metadata": {
        "id": "VT57xstfUtAL"
      },
      "source": [
        "Use the loaded metrics to compare the RAG pipelines from the previous subtask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M4SU_Z4vPAxy",
      "metadata": {
        "id": "M4SU_Z4vPAxy"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "### your code ###\n",
        "bleu =\n",
        "rouge =\n",
        "bertscore =\n",
        "### your code ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ET3Ns8NSQ73F",
      "metadata": {
        "id": "ET3Ns8NSQ73F"
      },
      "source": [
        "As seen in the previous subtask, the answers can contain multiple lines. To be able to compare the output of our systems to the gold answers, merge the multiple answers into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ITKpzWkQ_hJ",
      "metadata": {
        "id": "6ITKpzWkQ_hJ"
      },
      "outputs": [],
      "source": [
        "answers_merged=[]\n",
        "### your code ###\n",
        "\n",
        "### your code ###\n",
        "print(len(answers_merged))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IOlhI3iV1p9M",
      "metadata": {
        "id": "IOlhI3iV1p9M"
      },
      "source": [
        "Compute the BLUE score for the simple RAG and compressor RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z9PgmMfLPA0a",
      "metadata": {
        "id": "Z9PgmMfLPA0a"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "bleu_simple =\n",
        "bleu_compressor =\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bleu_simple)\n",
        "print(\"Compressor:\")\n",
        "print(bleu_compressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BZI2_Jfqrgrc",
      "metadata": {
        "id": "BZI2_Jfqrgrc"
      },
      "source": [
        "What does the elements below in the output of the BLEU impelementation in Hugging Face mean? (do not copy and paste the documentation but write the implications behind each element!).\n",
        "\n",
        "\n",
        "\n",
        "1.   **precisions:** `your answer`\n",
        "2.   **brevity_penalty:** `your answer`\n",
        "3.   **translation_length:** `your answer`\n",
        "4.   **reference_length:** `your answer`\n",
        "5.   **length_ratio:** `your answer`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G-usgncEr7i9",
      "metadata": {
        "id": "G-usgncEr7i9"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "\n",
        "1.   **precisions:** precision of n-grams, which is calculated as the number of n-grams that appear in both the machine-generated translation and the reference translations divided by the total number of n-grams in the machine-generated translation.\n",
        "2.   **brevity_penalty:** is a penalty term that adjusts the score for translations that are shorter than the reference translations. It is calculated as min(1, (reference_length / translation_length)). It essentially penalizes generated translations that are too short compared to the closest reference length with an exponential decay.\n",
        "3.   **translation_length:**   is the total number of words in the machine-generated translation.\n",
        "4.   **reference_length:**  is the total number of words in the reference translations.\n",
        "5. **length_ratio:** ratio of the 3 and 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ot-jQzvqPA3J",
      "metadata": {
        "id": "Ot-jQzvqPA3J"
      },
      "outputs": [],
      "source": [
        "### your code ###\n",
        "rouge_simple =\n",
        "rouge_compressor =\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(rouge_simple)\n",
        "print(\"Compressor:\")\n",
        "print(rouge_compressor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IFE7RI-HsSGq",
      "metadata": {
        "id": "IFE7RI-HsSGq"
      },
      "source": [
        "What is the difference in variants of ROUGE (ROUGE-N, ROUGE-L, ROUGE-SUM)?\n",
        "\n",
        "`your answer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IJmntC2_sfkx",
      "metadata": {
        "id": "IJmntC2_sfkx"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "ROUGE measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. The most common n-grams used are unigrams, bigrams, and trigrams. ROUGE score calculates the recall of n-grams in the machine-generated summary by comparing them to the reference summaries.\n",
        "\n",
        "**ROUGE-N:** ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap. For example, ROUGE-1 (unigram) measures the overlap of single words, ROUGE-2 (bigram) measures the overlap of two-word sequences, and so on. ROUGE-N is often used to evaluate the grammatical correctness and fluency of generated text.\n",
        "\n",
        "**ROUGE-L:** ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS. ROUGE-L is often used to evaluate the semantic similarity and content coverage of generated text, as it considers the common subsequence regardless of word order.\n",
        "\n",
        "**ROUGE-S:** ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. ROUGE-S is often used to evaluate the coherence and local cohesion of generated text, as it captures the semantic similarity between adjacent words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zSF5zMOyPA5I",
      "metadata": {
        "id": "zSF5zMOyPA5I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "bertscore_simple_averaged={}\n",
        "bertscore_compressor_averaged={}\n",
        "### your code ###\n",
        "\n",
        "\n",
        "### your code ###\n",
        "print(\"Simple system:\")\n",
        "print(bertscore_simple_averaged)\n",
        "print(\"Compressor:\")\n",
        "print(bertscore_compressor_averaged)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LfiuFxqQVrHa",
      "metadata": {
        "id": "LfiuFxqQVrHa"
      },
      "source": [
        "Which model works better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbIA8Cg3Zq6g",
      "metadata": {
        "id": "bbIA8Cg3Zq6g"
      },
      "source": [
        "#### ${\\color{red}{Comments\\ 2.2}}$\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ begin⚠️}}$\n",
        "\n",
        "\n",
        "```\n",
        "cross-feedback comment section\n",
        "```\n",
        "\n",
        "\n",
        "${\\color{red}{⚠️Comments\\ end⚠️}}$"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
